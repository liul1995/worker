{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import h5py\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./20170120.hdf5', './20170123.hdf5', './20170124.hdf5', './20170125.hdf5', './20170224.hdf5', './20170227.hdf5', './20170228.hdf5', './20170330.hdf5', './20170331.hdf5']\n",
      "./20170120.hdf5\n",
      "20170120\n",
      "./20170123.hdf5\n",
      "20170123\n",
      "./20170124.hdf5\n",
      "20170124\n",
      "./20170125.hdf5\n",
      "20170125\n",
      "./20170224.hdf5\n",
      "20170224\n",
      "./20170227.hdf5\n",
      "20170227\n",
      "./20170228.hdf5\n",
      "20170228\n",
      "./20170330.hdf5\n",
      "20170330\n",
      "./20170331.hdf5\n",
      "20170331\n",
      "23040\n",
      "23040\n",
      "{'20170123', '20170227', '20170125', '20170224', '20170124', '20170120', '20170331', '20170228', '20170330'}\n",
      "pre_rank\n",
      "0    0.004392\n",
      "1    0.005366\n",
      "2    0.005430\n",
      "3    0.004440\n",
      "4    0.003785\n",
      "5    0.004461\n",
      "6    0.003097\n",
      "7    0.007257\n",
      "8    0.001486\n",
      "9    0.003996\n",
      "Name: pct, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul\n",
    "        u = u / self.scale # 2.Scale\n",
    "\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf) # 3.Mask\n",
    "\n",
    "        attn = self.softmax(u) # 4.Softmax\n",
    "        output = torch.bmm(attn, v) # 5.Output\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "\n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "\n",
    "        batch, n_q, d_q_ = q.size()\n",
    "        batch, n_k, d_k_ = k.size()\n",
    "        batch, n_v, d_v_ = v.size()\n",
    "\n",
    "        q = self.fc_q(q) # 1.单头变多头\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出\n",
    "\n",
    "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
    "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" Self-Attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o, l):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "\n",
    "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
    "        self.rnn = nn.LSTM(d_o, l)\n",
    "        self.out = nn.Linear(l, 1)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "\n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        output, (hn, cn) = self.rnn(output)\n",
    "        output = self.out(output[:, :, -1])\n",
    "\n",
    "        return attn, output\n",
    "\n",
    "def get_file_list(folder):\n",
    "    filetype = 'hdf5'\n",
    "    filelist = []\n",
    "    for dirpath,dirnames,filenames in os.walk(folder):\n",
    "        for file in filenames:\n",
    "            filename = file.split('.')[0][:4]\n",
    "            file_type = file.split('.')[-1]\n",
    "            if file_type == filetype and filename in ['2017']:\n",
    "                file_fullname = os.path.join(dirpath, file) #文件全名\n",
    "                filelist.append(file_fullname)\n",
    "    return filelist\n",
    "\n",
    "def make_val_loader(file_path):\n",
    "    # start = time.clock()\n",
    "    temp=h5py.File(file_path,\"r\")\n",
    "    x_data = temp['vol'][()]\n",
    "    y_data = temp['pct_change'][()]\n",
    "    \n",
    "    x_data= torch.from_numpy(x_data).float().sum(axis=3)\n",
    "    y_data = torch.from_numpy(y_data).float()\n",
    "    dataset = TensorDataset(x_data,y_data)\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset,batch_size=256,shuffle=True,drop_last=True,pin_memory=True,num_workers=16)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ########\n",
    "    n_x = 31\n",
    "    d_x = 601\n",
    "    batch = 256\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "    mask = None\n",
    "\n",
    "    model = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=601, d_o=80, l=31)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load('./0.0777,0.0775.pkl'),strict=False)\n",
    "    \n",
    "#     filelist = get_file_list(r'/data1/lanwei/chouma_h5')\n",
    "    prefilelist = get_file_list(r'./')\n",
    "    print(prefilelist)\n",
    "#     val_dates_2=['20170103.hdf5','20170104.hdf5','20170105.hdf5','20170106.hdf5','20170109.hdf5','20170110.hdf5','20170111.hdf5','20170112.hdf5','20170113.hdf5','20170116.hdf5','20170117.hdf5','20170118.hdf5','20170119.hdf5','20170120.hdf5','20170123.hdf5','20170124.hdf5','20170125.hdf5','20170126.hdf5','20170203.hdf5','20170206.hdf5','20170207.hdf5','20170208.hdf5','20170209.hdf5','20170210.hdf5','20170213.hdf5','20170214.hdf5','20170215.hdf5','20170216.hdf5','20170217.hdf5','20170220.hdf5','20170221.hdf5','20170222.hdf5','20170223.hdf5','20170224.hdf5','20170227.hdf5','20170228.hdf5','20170301.hdf5','20170302.hdf5','20170303.hdf5','20170306.hdf5','20170307.hdf5','20170308.hdf5','20170309.hdf5','20170310.hdf5','20170313.hdf5','20170314.hdf5','20170315.hdf5','20170316.hdf5','20170317.hdf5','20170320.hdf5','20170321.hdf5','20170322.hdf5','20170323.hdf5','20170324.hdf5','20170327.hdf5','20170328.hdf5','20170329.hdf5','20170330.hdf5','20170331.hdf5']\n",
    "    predict = {'pre':[],'pct':[],'date':[]}\n",
    "    for path in prefilelist:\n",
    "        print(path)\n",
    "        date = path.split('/')[-1][:8]\n",
    "        print(date)\n",
    "        \n",
    "        pre = []\n",
    "        pct = []\n",
    "\n",
    "        val_loader = make_val_loader(path)\n",
    "        # print('loader2')\n",
    "        for k, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            attn, output = model(x)\n",
    "            \n",
    "            output = output.squeeze().cpu().detach().numpy()\n",
    "            y = y.squeeze().numpy()\n",
    "            pre.append(output)\n",
    "            pct.append(y)\n",
    "        predict['pre'].extend(pre)\n",
    "        predict['pct'].extend(pct)\n",
    "        predict['date'].extend([date]*(k+1)*batch)\n",
    "        \n",
    "    predict['pre'] = np.array(predict['pre']).flatten()\n",
    "    predict['pct'] = np.array(predict['pct']).flatten()\n",
    "    predict['date'] = np.array(predict['date']).flatten()\n",
    "    print(len(predict['pre']))\n",
    "    print(len(predict['pct']))\n",
    "    print(set(predict['date']))\n",
    "    \n",
    "    print\n",
    "    val_pre=pd.DataFrame(predict)\n",
    "    val_pre[\"pre_rank\"]=val_pre.groupby(\"date\")[\"pre\"].rank(pct=True)\n",
    "    val_pre[\"pre_rank\"]=pd.cut(val_pre[\"pre_rank\"],bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],labels=False)\n",
    "    print(val_pre.groupby(\"pre_rank\")[\"pct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre': array([0.5078752 , 0.50619495, 0.5111131 , ..., 0.4989614 , 0.50712484,\n",
       "        0.48038983], dtype=float32),\n",
       " 'pct': array([-0.01265821, -0.02297994, -0.01056455, ...,  0.01494767,\n",
       "        -0.07352945,  0.01472071], dtype=float32),\n",
       " 'date': array(['20170224', '20170224', '20170224', ..., '20170331', '20170331',\n",
       "        '20170331'], dtype='<U8')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12800.000000\n",
       "mean         0.498137\n",
       "std          0.011355\n",
       "min          0.459635\n",
       "25%          0.489291\n",
       "50%          0.502017\n",
       "75%          0.507875\n",
       "max          0.528686\n",
       "Name: pre, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pre[\"pre\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre': array([ 1,  3,  4,  1,  3,  5,  5,  6,  7,  8,  9, 10])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dit = dict()\n",
    "dit['pre'] = np.array([[[1,3,4],[1,3,5]],[[5,6,7],[8,9,10]]])\n",
    "dit['pre'] = np.array(dit['pre']).flatten()\n",
    "dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  4,  1,  3,  5,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,3,4],[1,3,5]],[[5,6,7],[8,9,10]]])\n",
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 4.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,3,4],[1,2,5]])\n",
    "a.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_loader(file_path):\n",
    "    # start = time.clock()\n",
    "    temp=h5py.File(file_path,\"r\")\n",
    "    x_data = temp['vol'][()]\n",
    "    y_data = temp['pct_change'][()]\n",
    "    \n",
    "    x_data= torch.from_numpy(x_data).float().sum(axis=3)\n",
    "    y_data = torch.from_numpy(y_data).float()\n",
    "    dataset = TensorDataset(x_data,y_data)\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset,batch_size=256,shuffle=True,drop_last=True,pin_memory=True,num_workers=16)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = make_val_loader('./20150105.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(index):\n",
    "    model=get_model(15,601)\n",
    "    model.load_weights(f\"/home/work/code/quant2/wyq/checkpoint_vol_601_15/{index}.h5\")\n",
    "    pre=model.predict(val2[0],batch_size=512)\n",
    "    val_pre=pandas.DataFrame({\"pre0\":pre[:,0],\"pct_change\":val2[2][:,0],\"trade_date\":val2[3]})\n",
    "    val_pre[\"pre_rank\"]=val_pre.groupby(\"trade_date\")[\"pre0\"].rank(pct=True)\n",
    "    val_pre[\"pre_rank\"]=pandas.cut(val_pre[\"pre_rank\"],bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],labels=False)\n",
    "    print(index,val_pre.groupby(\"pre_rank\")[\"pct_change\"].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
