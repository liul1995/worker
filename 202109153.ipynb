{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./20170120.hdf5', './20170123.hdf5', './20170124.hdf5', './20170125.hdf5', './20170224.hdf5', './20170227.hdf5', './20170228.hdf5', './20170330.hdf5', './20170331.hdf5']\n",
      "./20170120.hdf5\n",
      "20170120\n",
      "./20170123.hdf5\n",
      "20170123\n",
      "./20170124.hdf5\n",
      "20170124\n",
      "./20170125.hdf5\n",
      "20170125\n",
      "./20170224.hdf5\n",
      "20170224\n",
      "./20170227.hdf5\n",
      "20170227\n",
      "./20170228.hdf5\n",
      "20170228\n",
      "./20170330.hdf5\n",
      "20170330\n",
      "./20170331.hdf5\n",
      "20170331\n",
      "23040\n",
      "23040\n",
      "23040\n",
      "pre_rank\n",
      "0    0.003706\n",
      "1    0.004262\n",
      "2    0.004552\n",
      "3    0.003993\n",
      "4    0.005244\n",
      "5    0.004384\n",
      "6    0.004826\n",
      "7    0.005129\n",
      "8    0.003952\n",
      "9    0.003956\n",
      "Name: pct, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(n_hidden * 2, num_classes)\n",
    "\n",
    "    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "#         return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        return context\n",
    "    def forward(self, X):\n",
    "        input = x\n",
    "#         input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n",
    "        #len_seq=time embedding_dim=vol\n",
    "        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n",
    "        #1层双头\n",
    "        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)).to(device) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        cell_state = Variable(torch.zeros(1*2, len(X), n_hidden)).to(device) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
    "        attn_output= self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output) # model : [batch_size, num_classes], attention : [batch_size, n_step]\n",
    "\n",
    "def get_file_list(folder):\n",
    "    filetype = 'hdf5'\n",
    "    filelist = []\n",
    "    for dirpath,dirnames,filenames in os.walk(folder):\n",
    "        for file in filenames:\n",
    "            filename = file.split('.')[0][:4]\n",
    "            file_type = file.split('.')[-1]\n",
    "            if file_type == filetype and filename in ['2017']:\n",
    "                file_fullname = os.path.join(dirpath, file) #文件全名\n",
    "                filelist.append(file_fullname)\n",
    "    return filelist\n",
    "\n",
    "def make_val_loader(file_path):\n",
    "    # start = time.clock()\n",
    "    temp=h5py.File(file_path,\"r\")\n",
    "    x_data = temp['vol'][()]\n",
    "    y_data = temp['pct_change'][()]\n",
    "    \n",
    "    x_data= torch.from_numpy(x_data).float().sum(axis=3)\n",
    "    y_data = torch.from_numpy(y_data).float()\n",
    "    dataset = TensorDataset(x_data,y_data)\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset,batch_size=256,shuffle=True,drop_last=True,pin_memory=True,num_workers=16)\n",
    "\n",
    "    return loader\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ########\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "    embedding_dim = 601\n",
    "    n_hidden = 100\n",
    "    num_classes = 1\n",
    "\n",
    "    model = BiLSTM_Attention()\n",
    "    model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load('./0.0920,0.0889.pkl'),strict=False)\n",
    "    \n",
    "#     filelist = get_file_list(r'/data1/lanwei/chouma_h5')\n",
    "    prefilelist = get_file_list(r'./')\n",
    "    print(prefilelist)\n",
    "#     val_dates_2=['20170103.hdf5','20170104.hdf5','20170105.hdf5','20170106.hdf5','20170109.hdf5','20170110.hdf5','20170111.hdf5','20170112.hdf5','20170113.hdf5','20170116.hdf5','20170117.hdf5','20170118.hdf5','20170119.hdf5','20170120.hdf5','20170123.hdf5','20170124.hdf5','20170125.hdf5','20170126.hdf5','20170203.hdf5','20170206.hdf5','20170207.hdf5','20170208.hdf5','20170209.hdf5','20170210.hdf5','20170213.hdf5','20170214.hdf5','20170215.hdf5','20170216.hdf5','20170217.hdf5','20170220.hdf5','20170221.hdf5','20170222.hdf5','20170223.hdf5','20170224.hdf5','20170227.hdf5','20170228.hdf5','20170301.hdf5','20170302.hdf5','20170303.hdf5','20170306.hdf5','20170307.hdf5','20170308.hdf5','20170309.hdf5','20170310.hdf5','20170313.hdf5','20170314.hdf5','20170315.hdf5','20170316.hdf5','20170317.hdf5','20170320.hdf5','20170321.hdf5','20170322.hdf5','20170323.hdf5','20170324.hdf5','20170327.hdf5','20170328.hdf5','20170329.hdf5','20170330.hdf5','20170331.hdf5']\n",
    "    predict = {'pre':[],'pct':[],'date':[]}\n",
    "    for path in prefilelist:\n",
    "        print(path)\n",
    "        date = path.split('/')[-1][:8]\n",
    "        print(date)\n",
    "        \n",
    "        pre = []\n",
    "        pct = []\n",
    "\n",
    "        val_loader = make_val_loader(path)\n",
    "        # print('loader2')\n",
    "        for k, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            output = model(x)\n",
    "            \n",
    "            output = output.squeeze().cpu().detach().numpy()\n",
    "            y = y.squeeze().numpy()\n",
    "            pre.append(output)\n",
    "            pct.append(y)\n",
    "        predict['pre'].extend(pre)\n",
    "        predict['pct'].extend(pct)\n",
    "        predict['date'].extend([date]*(k+1)*256)\n",
    "        \n",
    "    predict['pre'] = np.array(predict['pre']).flatten()\n",
    "    predict['pct'] = np.array(predict['pct']).flatten()\n",
    "    predict['date'] = np.array(predict['date']).flatten()\n",
    "    print(len(predict['pre']))\n",
    "    print(len(predict['pct']))\n",
    "    print(len(predict['date']))\n",
    "    \n",
    "    print\n",
    "    val_pre=pd.DataFrame(predict)\n",
    "    val_pre[\"pre_rank\"]=val_pre.groupby(\"date\")[\"pre\"].rank(pct=True)\n",
    "    val_pre[\"pre_rank\"]=pd.cut(val_pre[\"pre_rank\"],bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],labels=False)\n",
    "    print(val_pre.groupby(\"pre_rank\")[\"pct\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
